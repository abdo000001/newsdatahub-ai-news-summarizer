{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI News Summarizer: NewsDataHub + OpenAI\n",
    "\n",
    "This notebook demonstrates how to build an AI-powered news summarization pipeline using:\n",
    "- **NewsDataHub API** ‚Äî For fetching news articles with comprehensive metadata\n",
    "- **OpenAI GPT-4o-mini** ‚Äî For generating concise, abstractive summaries\n",
    "\n",
    "**‚ö†Ô∏è Important Note About AI Accuracy:** AI-generated summaries may occasionally contain inaccuracies, omit important details, or misinterpret nuanced information. Always review AI outputs for critical applications.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "Install required packages (run once):\n",
    "```bash\n",
    "pip install requests openai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set your API keys and parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Keys\n",
    "NDH_API_KEY = \"\"  # NewsDataHub API key (leave empty to use sample data)\n",
    "OPENAI_API_KEY = \"your_openai_api_key_here\"  # Required for summarization\n",
    "\n",
    "# Configuration\n",
    "MIN_CONTENT_LENGTH = 300  # Minimum characters for article content\n",
    "NUM_ARTICLES_TO_PROCESS = 5  # Number of articles to summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Fetch News Articles from NewsDataHub\n",
    "\n",
    "We'll fetch English news articles from NewsDataHub. If no API key is provided, the code automatically downloads sample data from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if NewsDataHub API key is provided\n",
    "if NDH_API_KEY and NDH_API_KEY != \"your_ndh_api_key_here\":\n",
    "    print(\"Using live NewsDataHub API data...\")\n",
    "\n",
    "    url = \"https://api.newsdatahub.com/v1/news\"\n",
    "    headers = {\"x-api-key\": NDH_API_KEY}\n",
    "\n",
    "    # Fetch 100 English articles (single page, no pagination)\n",
    "    params = {\n",
    "        \"per_page\": 100,\n",
    "        \"language\": \"en\",  # English articles only\n",
    "        \"country\": \"US,GB,CA,AU\",  # English-speaking countries\n",
    "        \"source_type\": \"mainstream_news,digital_native\"  # Quality sources\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "\n",
    "    articles = data.get(\"data\", [])\n",
    "    print(f\"Fetched {len(articles)} English articles from NewsDataHub API\")\n",
    "\n",
    "else:\n",
    "    print(\"No NewsDataHub API key provided. Loading sample data...\")\n",
    "\n",
    "    # Download sample data if not already present\n",
    "    sample_file = \"sample-news-data.json\"\n",
    "\n",
    "    if not os.path.exists(sample_file):\n",
    "        print(\"Downloading sample data from GitHub...\")\n",
    "        sample_url = \"https://raw.githubusercontent.com/newsdatahub/newsdatahub-ai-news-summarizer/refs/heads/main/data/sample-news-data.json\"\n",
    "        response = requests.get(sample_url)\n",
    "        response.raise_for_status()\n",
    "        with open(sample_file, \"w\") as f:\n",
    "            json.dump(response.json(), f)\n",
    "        print(f\"Sample data saved to {sample_file}\")\n",
    "\n",
    "    # Load sample data\n",
    "    with open(sample_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Handle both formats: raw array or API response with 'data' key\n",
    "    if isinstance(data, dict) and \"data\" in data:\n",
    "        articles = data[\"data\"]\n",
    "    elif isinstance(data, list):\n",
    "        articles = data\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected sample data format\")\n",
    "\n",
    "    print(f\"Loaded {len(articles)} articles from sample data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Filter Articles with Sufficient Content\n",
    "\n",
    "Remove articles with minimal content (photo galleries, breaking alerts, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter articles with sufficient content\n",
    "filtered_articles = [\n",
    "    article for article in articles\n",
    "    if article.get(\"content\") and len(article.get(\"content\", \"\")) >= MIN_CONTENT_LENGTH\n",
    "]\n",
    "\n",
    "print(f\"Filtered {len(filtered_articles)} articles with content >= {MIN_CONTENT_LENGTH} characters\")\n",
    "print(f\"Removed {len(articles) - len(filtered_articles)} articles with insufficient content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Initialize OpenAI Client and Summarization Function\n",
    "\n",
    "Create a reusable function for generating AI summaries using GPT-4o-mini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def summarize_article(content, title):\n",
    "    \"\"\"\n",
    "    Generate an abstractive summary of a news article using OpenAI GPT-4o-mini.\n",
    "\n",
    "    Args:\n",
    "        content (str): The full article content\n",
    "        title (str): The article title (provides context to the AI)\n",
    "\n",
    "    Returns:\n",
    "        str: A 2-3 sentence summary, or error message if summarization fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a professional news summarizer. Create concise, accurate 2-3 sentence summaries that capture the key information and main points of articles.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Summarize this news article in 2-3 sentences:\\n\\nTitle: {title}\\n\\nContent: {content}\"\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=150,  # ~100-150 words for 2-3 sentences\n",
    "            temperature=0.3  # Lower temperature for consistent, focused summaries\n",
    "        )\n",
    "\n",
    "        summary = response.choices[0].message.content.strip()\n",
    "        return summary\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error generating summary: {str(e)}\"\n",
    "\n",
    "print(\"‚úì Summarization function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test on a Single Article\n",
    "\n",
    "Let's test the summarization function on one article to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on the first filtered article\n",
    "test_article = filtered_articles[0]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TEST ARTICLE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTitle: {test_article.get('title', 'N/A')}\")\n",
    "print(f\"Source: {test_article.get('source_title', 'N/A')}\")\n",
    "print(f\"Published: {test_article.get('pub_date', 'N/A')}\")\n",
    "print(f\"Content length: {len(test_article.get('content', ''))} characters\")\n",
    "\n",
    "# Generate summary\n",
    "print(\"\\nGenerating AI summary...\")\n",
    "summary = summarize_article(\n",
    "    content=test_article.get(\"content\", \"\"),\n",
    "    title=test_article.get(\"title\", \"\")\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"AI SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Structured Output Function\n",
    "\n",
    "Combine NewsDataHub metadata with AI summaries in a structured format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_output(article, summary):\n",
    "    \"\"\"\n",
    "    Combine NewsDataHub article metadata with AI-generated summary.\n",
    "\n",
    "    Args:\n",
    "        article (dict): Original article from NewsDataHub\n",
    "        summary (str): AI-generated summary\n",
    "\n",
    "    Returns:\n",
    "        dict: Structured output with metadata and summary\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"id\": article.get(\"id\"),\n",
    "        \"title\": article.get(\"title\"),\n",
    "        \"source\": article.get(\"source_title\"),\n",
    "        \"published\": article.get(\"pub_date\"),\n",
    "        \"url\": article.get(\"article_link\"),\n",
    "        \"language\": article.get(\"language\"),\n",
    "        \"topics\": article.get(\"topics\", []),\n",
    "        \"original_content_length\": len(article.get(\"content\", \"\")),\n",
    "        \"ai_summary\": summary\n",
    "    }\n",
    "\n",
    "# Create structured output for test article\n",
    "output = create_summary_output(test_article, summary)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STRUCTURED OUTPUT (JSON)\")\n",
    "print(\"=\"*80)\n",
    "print(json.dumps(output, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Process Multiple Articles in Batch\n",
    "\n",
    "Now let's process 5 articles to demonstrate a production pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(f\"PROCESSING {NUM_ARTICLES_TO_PROCESS} ARTICLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summarized_articles = []\n",
    "\n",
    "for i, article in enumerate(filtered_articles[:NUM_ARTICLES_TO_PROCESS], 1):\n",
    "    print(f\"\\n[{i}/{NUM_ARTICLES_TO_PROCESS}] Processing: {article.get('title', 'N/A')[:60]}...\")\n",
    "\n",
    "    # Generate summary\n",
    "    summary = summarize_article(\n",
    "        content=article.get(\"content\", \"\"),\n",
    "        title=article.get(\"title\", \"\")\n",
    "    )\n",
    "\n",
    "    # Create structured output\n",
    "    output = create_summary_output(article, summary)\n",
    "    summarized_articles.append(output)\n",
    "\n",
    "    print(f\"    ‚úì Summary generated ({len(summary)} characters)\")\n",
    "\n",
    "print(f\"\\n‚úì Successfully processed {len(summarized_articles)} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Results to JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to JSON file\n",
    "output_file = \"summarized_articles.json\"\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(summarized_articles, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Results saved to {output_file}\")\n",
    "print(f\"  Total articles: {len(summarized_articles)}\")\n",
    "print(f\"  File size: {os.path.getsize(output_file):,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Display Summary Report\n",
    "\n",
    "Let's create a clean, readable summary report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, article in enumerate(summarized_articles, 1):\n",
    "    print(f\"\\nüì∞ Article {i}\")\n",
    "    print(f\"   Title: {article['title']}\")\n",
    "    print(f\"   Source: {article['source']} | Published: {article['published'][:10]}\")\n",
    "    print(f\"   Topics: {', '.join(article['topics']) if article['topics'] else 'N/A'}\")\n",
    "    print(f\"\\n   üìù AI Summary:\")\n",
    "    print(f\"   {article['ai_summary']}\\n\")\n",
    "    print(f\"   üîó Read full article: {article['url']}\")\n",
    "    print(f\"   {'-'*76}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Generated {len(summarized_articles)} AI summaries using NewsDataHub + OpenAI\")\n",
    "print(\"\\n‚ö†Ô∏è  Reminder: AI-generated summaries may occasionally contain inaccuracies.\")\n",
    "print(\"   Always review outputs for critical applications.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Estimation\n",
    "\n",
    "Let's estimate the OpenAI API cost for this batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üí∞ OpenAI API Cost Estimation\")\n",
    "print(\"=\"*80)\n",
    "print(\"GPT-4o-mini pricing:\")\n",
    "print(\"  Input tokens:  ~$0.15 per 1M tokens\")\n",
    "print(\"  Output tokens: ~$0.60 per 1M tokens\")\n",
    "print(f\"\\nFor {len(summarized_articles)} articles:\")\n",
    "print(\"  Approximate cost: < $0.01 (one cent)\")\n",
    "print(\"\\nSummarization with GPT-4o-mini is extremely affordable!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "**Expand the pipeline:**\n",
    "- Process more articles (change `NUM_ARTICLES_TO_PROCESS`)\n",
    "- Filter by specific topics or countries\n",
    "- Adjust summary length (`max_tokens` parameter)\n",
    "- Add retry logic for API failures\n",
    "- Implement caching to avoid redundant API calls\n",
    "\n",
    "**Learn more:**\n",
    "- [Full Tutorial](https://newsdatahub.com/learning-center/article/ai-summarization-pipeline)\n",
    "- [NewsDataHub API Docs](https://newsdatahub.com/docs)\n",
    "- [OpenAI API Docs](https://platform.openai.com/docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
